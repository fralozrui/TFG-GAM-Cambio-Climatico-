---
author: "Francisco José Lozano Ruiz"
date: "27/10/2017"
documentclass: book
forprint: true  # true: imprime a dos caras, false: libro digital
fontsize: 12pt # 10pt,11pt
geometry: margin = 2.5cm 
bibliography: ["bib/library.bib", "bib/paquetes.bib"]
# metodobib -> true: natbib (descomentar: citation_package: natbib) 
#           -> false: pandoc (comentar: citation_package: natbib)
metodobib: true
#natbib: plainnat, abbrvnat, unsrtnat
biblio-style: "plainnat"
#Método 2 (pandoc): descomente una línea de las 2 siguientes en caso de usarlo
csl: methods-in-ecology-and-evolution.csl      # no numera mejor en las citas
#csl: acm-sig-proceedings-long-author-list.csl  # numera peor en las citas
link-citations: yes
output: 
  pdf_document:
    keep_tex: no
    number_sections: yes
    citation_package: natbib  # comentado usa: pandoc-citeproc
    #toc: yes
    fig_caption: yes
    template: latex/templateMemoriaTFE.tex
    includes:
      #before_body: portadas/latex_paginatitulo_modTFE.tex
      #in_header: latex/latex_preambulo.tex
      #after_body: latex/latex_antes_enddoc.tex
---



```{r include=FALSE}
knitr::opts_chunk$set(fig.path = 'figurasR/',
                      echo = FALSE, warning = FALSE, message = FALSE,
                      fig.pos="H",fig.align="center",out.width="95%",
                      cache=FALSE)

```


<!-- \setcounter{chapter}{2} -->
<!-- \setcounter{chapter}{2} escribir 2 para capítulo 3  -->
<!-- \pagenumbering{arabic} -->

\ifdefined\ifprincipal
\else
\setlength{\parindent}{1em}
\pagestyle{fancy}
\setcounter{tocdepth}{4}
\tableofcontents
<!-- \nocite{*} -->
\fi

\ifdefined\ifdoblecara
\fancyhead{}{}
\fancyhead[LE,RO]{\scriptsize\rightmark}
\fancyfoot[LO,RE]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[LE,RO]{\footnotesize\thepage}
\else
\fancyhead{}{}
\fancyhead[RO]{\scriptsize\rightmark}
\fancyfoot[LO]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[RO]{\footnotesize\thepage}
\fi
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\theoremstyle{remark}
\newtheorem{observación}{Observación}[section]
\theoremstyle{definition}
\newtheorem{definición}{Definición}[section]
\theoremstyle{definición}
\newtheorem{proposición}{Proposición}[section]
\theoremstyle{remark}
\newtheorem{ejemplo}{Ejemplo}[section]

# Modelos aditivos generalizados

## Introducción

Como bien podemos intuir por su nombre, los modelos aditivos generalizados no son más que la fusión entre los modelos lineales generalizados y los modelos aditivos, los cuales se introducen con una sección en este capítulo. Podemos ver estos dos tipos de modelos como extensiones del modelo lineal. Por un lado, como vimos en el capítulo anterior, el MLG hace uso de una función de enlace entre el predictor lineal y el valor esperado de la variable dependiente para poder expresar relaciones más complejas y relaja la hipótesis distribucional permitiendo que tal variable siga distribuciones de la familia exponencial. Por otro lado, los modelos aditivos, además de también relajar esta hipótesis de distribución, introducen las funciones de suavizado en el modelo, estas proporcionan más flexibilidad a la hora de relacionar las variables explicativas con la de respuesta.

Luego, como ya hemos mencionado, y como se plantea en @Hastie, el MAG reune estas dos propuestas de modo que generaliza el modelo aditivo de la misma forma que el MLG generalizaba el modelo lineal. Sin embargo, la flexibilidad que proporciona este modelo da lugar a dos nuevos problemas teóricos: cómo estimar las funciones de suavizado y cómo de "suaves" deben ser. 

En este capítulo nos adentramos en los modelos no paramétricos, es decir, en aquellos que en vez de expresar la relación del valor esperado de la variable de respuesta con las variables predictoras mediante un predictor lineal, lo hacen mediante funciones $f$, como se vió en $\ref{eq:modelo general}$, pero ahora sin hacer ninguna suposición sobre ella. Esto conllevará en muchas ocasiones un mejor ajuste del modelo y traerá a la mesa una nueva cuestión conocida como sobreajuste que, aunque ya aparecía para los modelos paramétricos, ahora jugará un papel fundamental a la hora de querer predecir datos fuera de los observados. Este concepto refleja el hecho de que el modelo ajusta tan bien los datos proporcionados para la estimación de sus parámetros que es incapaz de mostrar la verdadera relación entre las variables que se estudian y, por tanto, da lugar a predicciones de nuevos datos que no serán las idóneas.

Tal y como se hace en @Wood, comenzaremos viendo cómo construir los modelos aditivos generalizados, es decir, qué bases de funciones podemos elegir para obtener las funciones de suavizado y qué parámetro de suavizado se debe seleccionar o cómo se puede estimar. Luego se introduce el modelo aditivo, en el que se utilizarán los resultados vistos a lo largo del capítulo. Tras todo ello se propone la forma final del modelo aditivo generalizado.


\begin{definición}[Estructura básica del modelo aditivo generalizado]
$$
\mu = \begin{pmatrix} \mu_1 \\ \vdots \\ \mu_n \end{pmatrix} = 
\begin{pmatrix} E[Y_1] \\ \vdots \\ E[Y_n] \end{pmatrix} = E[Y]
$$
\begin{equation}
g(\mu_i) = A_i\theta + f_1(x_{1i}) + f_2(x_{2i}) + f_3(x_{3i},x_{4i}) + \dots  \hspace{0.5cm}, 
\forall i =1,\dots,n
\label{eq:MAG}
\end{equation}
Donde:
\begin{itemize}
  \item $Y_i$ es la variable de respuesta y sigue una distribución de la familia exponencial de media $\mu_i$ y parámetro de escalado $\phi$. A partir de ahora esto lo denotaremos por: $Y_i \backsim EF(\mu_i,\phi)$.
  \item $A_i$ es la fila i-ésima de la matriz del modelo para aquellas componentes del modelo que son estrictamente paramétricas.
  \item $\theta$ es el correspondiente vector de parámetro, que antes denotábamos por $\beta$, para las variables predictoras mencionadas en el anterior punto.
  \item Las $f_i$ son las funciones de suavizado para las covariables $x_k$. Suelen ser desconocidas y el principal objetivo es el de estimarlas, pero también pueden darse casos, la mayoría de modelos biológicos, en los que son conocidas y nos interesa estimar otros parámetros del modelo.
\end{itemize}
\end{definición}


## Suavizado univariante

Dicho esto, partiremos considerando modelos que, aunque no sean adecuados para un uso práctico general, nos permitirán estudiar el marco teórico de una forma más sencilla. Es decir, en esta sección consideraremos un modelo con una sola función de suavizado, $f$, y una sola covariable, $x$, de la forma: 
\begin{equation}
y_i = f(x_i) + \epsilon_i
\label{eq: basic MAG}
\end{equation}
Donde $y_i$ es la variable de respuesta y los $\epsilon_i$ son variables aleatorias independientes e identicamente distribuidas como $N(0,\sigma^2)$ que representan el error.


### Bases de funciones

Nos proponemos en esta sección obtener una estimación de la función de suavizado a partir de una base de un espacio de funciones, en el que también se encontrará $f$ (o una aproximación suya). Elegir una base equivale a tomar un conjunto de funciones $\{b_j(x)\}_{j=1}^k$ y, por tanto, podemos representar la función de suavizado como:
\begin{equation}
f(x) = \sum_{j=1}^k b_j(x)\beta_j
\label{eq:base funcion suavizado}
\end{equation}
Para ciertos parámetros $\beta_j$ a determinar.

**Base polinómica** \newline
Si consideramos la base $\mathcal{B}$ del espacio de polinomios de grado k, es decir, $\mathcal{B} = \{1, x_i, x_i^2, \dots, x_i^k \}$, la función de suavizado toma la forma: 
$$
f(x) = \beta_1 + \beta_2 x + \beta_3 x^2 + \dots + \beta_{k+1} x^k
$$
Y, por tanto, el modelo $\ref{eq: basic MAG}$ queda:
$$
y_i = \beta_1 + \beta_2 x_i + \beta_3 x_i^2 + \dots + \beta_{k+1} x_i^k + \epsilon_i
$$

\begin{observación}[Problema de la base polinómica]
Notemos que por el teorema de Taylor, la base polinomial nos será útil cuando nuestro interés sea el de estudiar las propiedades de la función de suavizado en el entorno de un punto concreto, pero nos encontramos con problemas cuando queremos hacerlo en todo el dominio de $f$.

El principal problema se debe a que la interpolación de los datos puede resultar en una función muy oscilante o que no ajuste bien la información, dependiendo del valor de k, y que al modificar un coeficiente del modelo, el cambio impacta a los valores ajustados en todo el rango de la variable explicativa. Esto se puede solucionar de cierta manera con el siguiente tipo de base de funciones.
\end{observación}

**Base lineal por partes** \newline
Consideremos ahora una partición de nodos $\{x^*_j : j = 1,\dots,k \}$ del rango de la variable predictora $x$ tal que $x^*_j > x^*_{j+1}$ y la base de funciones $\mathcal{B} = \{b_j(x)\}_{j=1}^k$ donde:


$$
b_1(x) = \left\{
\begin{array}{ l }
\frac {x^*_2 - x} {x^*_2 - x^*_1} \hspace{0.5cm}, si \hspace{0.2cm} x < x^*_2  \\
0 \hspace{0.5cm} c.c.
\end{array}
\right. 
$$
$$
b_j(x) = \left\{
\begin{array}{ l }
\frac {x - x^*_{j-1}} {x^*_j - x^*_{j-1}} \hspace{0.5cm}, si \hspace{0.2cm} x^*_{j-1}<x < x^*_j  \\
\frac {x^*_{j+1} - x} {x^*_{j+1} - x^*_j} \hspace{0.5cm}, si \hspace{0.2cm} x^*_{j}<x < x^*_{j+1} \\
0 \hspace{0.5cm} c.c
\end{array}
\right.
$$

$$
b_k(x) = \left\{
\begin{array}{ l }
\frac {x - x^*_{k-1}} {x^*_k - x^*_{k-1}} \hspace{0.5cm}, si \hspace{0.2cm} x > x^*_{k-1}  \\
0 \hspace{0.5cm} c.c.
\end{array}
\right. 
$$
Es decir, la base de funciones $b_j(x)$ que son 0 en todo su dominio excepto entre los nodos a izquierda y derecha de $x_j^*$, donde crece y decrece de forma lineal hasta llegar a 1 en tal nodo. Este tipo de funciones se conocen como *tent functions*.

\begin{ejemplo}
Supongamos que el rango de $x$ va de 0 a 5 y consideremos 6 nodos: $\{0,1,2,3,4,5\}$, entonces podemos representar las funciones $b_0(x)$, $b_2(x)$ y $b_5(x)$ como:
```{r}
b0 <- function(x) {
  ifelse(x < 1, 1-x, 0)
}

b2 <- function(x) {
  ifelse(1< x & x < 2, x-1, ifelse(2<x & x<3,3-x,0))
}

b5 <- function(x){
  ifelse(x > 4, x-4, 0)
}

x <- seq(0, 5, length.out = 100)
y0 <- b0(x)
y2 <- b2(x)
y5 <- b5(x)

par(mfrow = c(1, 3))

plot(x, y0, type = "l", col = "blue", main = "Función básica b0(x)")
abline(v = 0, lty = 2, col = "green")
points(0, 0, col = "red", pch = 16)
text(0, 0, "x0", pos = 3)
plot(x, y2, type = "l", col = "blue", main = "Función básica b2(x)")
abline(v = 2, lty = 2, col = "green")
points(2, 0, col = "red", pch = 16)
text(2, 0, "x2", pos = 3)
plot(x, y5, type = "l", col = "blue", main = "Función básica b5(x)")
abline(v = 5, lty = 2, col = "green")
points(5, 0, col = "red", pch = 16)
text(5, 0, "x5", pos = 3)

```
\end{ejemplo}

De momento sólo planteamos estas formas de estimar las funciones de suavizado para tener una idea inicial y sencilla de cómo hacerlo pero más adelante dedicamos una sección a mejorar estas estimaciones mediante *splines*.

### Control del suavizado

Nos interesará ahora controlar el grado de suavizado del GAM. Para ello tendremos en cuenta que el modelo aproxime de forma correcta los datos a la vez que la curvatura se mantiene controlada. Consideramos un nuevo parámetro $\lambda$, denominado parámetro de suavizado, el cuál tiene como principal función el compensar entre la fidelidad a los datos del modelos y el grado de suavizado del mismo.

Notemos primero que podemos representar la penalización a la curvatura de $f$ como: 
$$
\int(f'')^2
$$
Y en el caso de utilizar la base de funciones lineales por partes se puede aproximar[^1] por:
$$
\sum_{j=2}^{k-1}(f(x_{j-1}^*)-2f(x_j^*)+f(x_{j+1}^*))^2
$$

Es fácil observar que cuando $f$ es una línea recta la penalización es 0 y cuando presenta muchas fluctuaciones en su curvatura este término es mayor.

[^1]: Se supone que se los nodos están espaciados de manera uniforme, pues en el caso de no que no lo estuvieran habría que añadir pesos a la suma.

Luego, en vez de ajustar el modelo por mínimos cuadrados, ahora se hará añadiendo la anterior penalización, es decir, minimizando: 
\begin{equation}
||y - X\beta||^2 + \lambda \sum_{j=2}^{k-1}(f(x_{j-1}^*)-2f(x_j^*)+f(x_{j+1}^*))^2
\label{pen-least-squares}
\end{equation}

\begin{observación}
Mientras mayor sea $\lambda$ más importancia le estaremos dando a que la función $f$ sea suave y menos a que a aproxime bien los datos. De hecho, cuando $\lambda \rightarrow \infty $ la función de suavizado $f$ que minimiza la anterior expresión será una línea recta y cuando $\lambda=0$ resultará en una estimación no penalizada. 
\end{observación}

### Elección del parámetro de suavizado

Cómo hemos visto en la observación anterior: si el parámetro de suavizado es muy grande, el modelo será demasiado simple como para ajustarse bien a los datos y si es muy pequeño, la función de suavizado tendrá una curvatura muy alta. En cualquiera de los casos se tendrá que la estimación de $f$ no se parecerá a la función real que ajusta los datos. Por ello, debemos dar un criterio para la elección de $\lambda$.

Un primer criterio planteado en @Wood es el de elegir $\lambda$ de forma que minimice la siguiente expresión para $x_1,\dots,x_n$ unas observaciones dadas.
$$
M = \frac{1}{n} \sum_{i=1}^n(\hat{f_i}-f_i)^2
$$
Donde $\hat{f_i} = \hat{f}(x_i)$ es la evaluación de los puntos dados en la estimación de la función y $f_i = f(x_i)$ son sus evaluaciones en la función real.

Sin embargo, como la función $f$ es desconocida, no es posible utilizar este criterio directamente. Daremos entonces una primera versión **método de validación cruzada**.

\begin{definición}[Validación cruzada ordinaria]
Sea $\hat{f_i}^{[-i]}$ la estimación de la función de suavizado que ajustada por todos los datos $\{(x_j,y_j)\}_{j=1}^n$ menos el i-ésimo, se define la validación cruzada ordinaria como:
\begin{equation}
\nu_0 = \frac{1}{n} \sum_{i=1}^n(\hat{f_i}^{[-i]}-y_i)^2
\label{cross-val-ord}
\end{equation}

\end{definición}

Se puede entender como que se ajusta el modelo sin utilizar la observación $(x_i,y_i)$, se predice la variable de respuesta con este modelo en el punto $x_i$ y luego se calcula la diferencia al cuadrado entre la estimación y el valor observado $\forall i=1,\dots ,n$.

\begin{observación}
Podemos ver que tomar $\lambda$ de modo que minimice $\nu_0$ es una buena manera de abordar que minimice M. Para ello veamos que $E[\nu_0] \approx E[M] + \sigma^2$. Sustituyendo en $\ref{cross-val-ord}$ que $y_i = f_i + \epsilon_i$ nos queda que:
$$
\nu_0 = \frac{1}{n} \sum_{i=1}^n(\hat{f_i}^{[-i]}-f_i + \epsilon_i)^2 = 
        \frac{1}{n} \sum_{i=1}^n[(\hat{f_i}^{[-i]}-f_i)2 - 2(\hat{f_i}^{[-i]}-f_i)\epsilon_i + \epsilon_i^2]
$$
Entonces, tomando valor esperado y teniendo en cuenta que $E[\epsilon_i]=0$ y que $\epsilon_i$ y $f_i$ son independientes:
$$
E[\nu_0] = \frac{1}{n} E[\sum_{i=1}^n(\hat{f_i}^{[-i]}-f_i)^2] + \sigma^2 = E[M] + \sigma^2
$$
Por lo tanto, cuando $n \rightarrow \infty$ se tienen las igualdades $E[\nu_0] = E[M] + \sigma^2$ y $\hat{f}^{[-i]} = \hat{f}$.
\end{observación}

Si los modelos sólo fueran juzgados por su capacidad de ajustar los datos que les aportamos, entonces siempre se elegirían los modelos más complejos, pero el elegir el modelo que maximice la capacidad de predecir nuevos datos no tiene este problema.

Sin embargo, como se indica en @Wood, p.171, este método es costoso computacionalmente, ya que se deben realizar n ajustes de los datos, por ello se propone un nuevo método el cuál hace uso de la matriz de influencia $A$.

\begin{definición}[Validación cruzada generalizada]
Dadas unas observaciones $\{(x_i,y_i)\}_{i=1}^n$ se elige $\lambda$ tal que minimice:
$$
\nu_g = n \frac{\sum_{i=1}^n(y_i-\hat{f_i})^2}{(n-tr(A))^2}
$$
\end{definición}

## Modelos aditivos

Como ya hemos mencionado previamente, el modelo aditivo es una extensión del modelo de regresión lineal. Su principal característica, la cual da lugar a su nombre, es que los efectos de las variables predictoras sobre la variable de respuesta son aditivos, es decir, una vez ajustado el modelo aditivo se pueden examinar tales efectors por separado. Veremos primero la forma general del modelo aditivo tal y como la introduce @Hastie y luego desarrollaremos la teoría alrededor de él para el caso de dos variables predictoras.

\begin{definición}[Modelo aditivo]
Supongamos el contexto de las anteriores de las definiciones de modelos, el modelo aditivo se expresa como:
$$
Y = \alpha + \sum_{j=1}^p f_j(X_j) + \epsilon
$$
Donde $\alpha$ es el término independiente, $\epsilon$ son errores aleatorios independientes de los $X_j$ tales que $E[\epsilon] = 0$ y $Var(\epsilon)= \sigma^2$, y las $f_j$ son funciones que conviene suponer univariadas y suaves pero no es necesario.
\end{definición}

\begin{observación}
Además se debe tener que $E[f_j(X_j)]=0 \hspace{0.5cm} \forall j =1,\dots,n$, pues de otro modo las funciones $f_j$ añadirían términos independientes constantes adicionales.
\end{observación}

Suele ser útil el pensar el modelo aditivo como un método que primero estima los parámetros adecuados en los que medir las variables y luego realiza el análisis lineal estándar sobre las variables transformadas. La principal motivación a priori tras este tipo de modelos es que, al representar por separado el efecto de cada variable predictora, mantienen la interpretabilidad del modelo lineal.

En lo que sigue, para poder ajustar más fácilmente el modelo como en @Wood, supondremos que se tienen sólo dos variables predictoras $X = (X_1,\dots,X_p)$ y $V = (V_1,\dots,V_p)$ y consideraremos el modelo aditivo:
\begin{equation}
y_i = \alpha + f_1(x_i) + f_2(v_i) + \epsilon_i
\label{modelo aditivo}
\end{equation}

\begin{observación}
Los principales problemas del modelo aditivo son:
\begin{itemize}
  \item La suposición de los efectos aditivos sobre $\ref{eq:modelo general}$ es bastante restrictiva.
  \item Existen problemas de identificabilidad pues las $f_j$ son estimables con una precisión de una constante aditiva.
\end{itemize}
Sin embargo, si suponemos resueltos estos problemas, el modelo aditivo puede ser representado por splines de regresión penalizados, los cuales serán estimados mediante mínimos cuadrados penalizados, y el grado de suavizado, que se obtendrá por validación cruzada.
\end{observación}

**Modelo aditivo por regresión penalizada por partes**

En lo que sigue, consideraremos la base del espacio de funciones lineales por partes vista en la sección anterior, es decir, expresamos las funciones $f_1$ y $f_2$ como:
$$
f_1(x) = \sum_{j=1}^{k_1} b_j(x)\delta_j \\
f_2(v) = \sum_{j=1}^{k_2} \beta_j(v)\gamma_j
$$
Donde $\delta_j$ y $\gamma_j$ son parámetros conocidos y las $b_j$ y $\beta_j$ son las funciones básicas de tipo carpa para los nodos $x^*_j$ y $v^*_j$ respectivamente, los cuales están espaciados uniformemente en el rango de $x$ y $v$. 

Definimos ahora los vectores n-dimesionales $\vec{f_1} = (f_1(x_1),\dots,f_1(x_n))^T$ y $\vec{f_2} = (f_2(v_1),\dots,f_2(v_n))^T$ como:
$$
\vec{f_1} = X_1\delta = 
\begin{pmatrix}
b_1(x_1) & \dots & b_{k1}(x_1) \\ \vdots & \dots & \vdots \\ b_1(x_n) & \dots & b_{k1}(x_n)
\end{pmatrix}
\begin{pmatrix} 
\delta_1 \\ \vdots \\ \delta_{k1}
\end{pmatrix} = 
\begin{pmatrix}
\sum_{j=1}^{k1} b_j(x_1)\delta_j \\ \vdots \\ \sum_{j=1}^{k1} b_j(x_n)\delta_j
\end{pmatrix} = 
\begin{pmatrix}
f_1(x_1) \\ \vdots \\ f_1(x_n)
\end{pmatrix}
$$
$$
\vec{f_2} = X_2\gamma = 
\begin{pmatrix}
\beta_1(v_1) & \dots & \beta_{k2}(v_1) \\ \vdots & \dots & \vdots \\ \beta_1(v_n) & \dots & \beta_{k2}(v_n)
\end{pmatrix}
\begin{pmatrix} 
\gamma_1 \\ \vdots \\ \gamma_{k1}
\end{pmatrix} = 
\begin{pmatrix}
\sum_{j=1}^{k2} \beta_j(v_1)\gamma_j \\ \vdots \\ \sum_{j=1}^{k2} \beta_j(v_n)\gamma_j
\end{pmatrix} = 
\begin{pmatrix}
f_2(v_1) \\ \vdots \\ f_2(v_n)
\end{pmatrix}
$$

\begin{proposición}
En el caso de considerar la base lineal por partes, los coeficientes $\beta_j$ que definen a una función $f$ coinciden con los valores de la función en los nodos, es decir, $\beta_j = f(x^*_j)$.
\end{proposición}

Gracias a esto, se tiene que el problema de ajuste de la regresión penalizada se reduce a minimizar la siguiente expresión respecto de $beta$: 
$$
||y-X\beta||^2 + \lambda\beta^TS\beta
$$
Donde $S = D^TD$ con $D = \begin{pmatrix} 1 & -2 & 1 & 0 & \dots & \dots \\ 0 & 1 & -2 & 1 & 0 & \dots \\ 0 & 0 & 1 & -2 & 1 &  \dots \\  \dots & \dots & \dots & \dots & \dots & \ddots\end{pmatrix}$.

Por lo tanto, la penalización asociada a las funciones $f_1$ y $f_2$ vienen dadas por:
$$
\left\{
\begin{array}{ l }
\delta^TD_1^TD_1\delta = \delta S_1\delta \\
\gamma^TD_2^TD_2\gamma = \gamma S_2 \gamma
\end{array}
\right.
$$

Además, para tratar el problema de identificabilidad utilizaremos la siguiente restricción lineal:
$$
\left\{
\begin{array}{ l }
\sum_{i=1}^nf_1(x_i) = 0 \Leftrightarrow \bar{1}^T \vec{f_1} = 0 \Leftrightarrow \bar{1}^TX\delta = 0 \hspace{0.3cm} \forall \delta \Leftrightarrow \bar{1}^TX = 0 \\ 
\sum_{i=1}^nf_2(v_i) = 0 \Leftrightarrow \bar{1}^T \vec{f_2} = 0 \Leftrightarrow \bar{1}^TV\gamma = 0 \hspace{0.3cm} \forall \gamma \Leftrightarrow \bar{1}^TV = 0
\end{array}
\right.
$$
Donde $\bar{1}$ es un vector n-dimensional con todas las componentes iguales a 1. Ahora bien, para que se pueda cumplir esta condición debemos retirar de cada columna de las marices X y V la media de tales columnas, es decir, definiremos las nuevas matrices centradas por columnas y las respectivas transformaciones de $f_1$ y $f_2$: 
$$
\left\{
\begin{array}{ l }
\tilde{X} = X - \bar{1} \bar{1}^T \frac{X}{n} \hspace{0.3cm},\hspace{0.3cm}\tilde{f_1} = \tilde{X}\delta \\ 
\tilde{V} = V - \bar{1} \bar{1}^T \frac{V}{n} \hspace{0.3cm},\hspace{0.3cm}\tilde{f_2} = \tilde{V}\gamma
\end{array}
\right.
$$
\begin{observación}
Esta nueva restricción y la transformación de las funciones no afecta a las restricciones impuestas con anterioridad, de hecho solo implica un cambio constante en las funciones: 
$$
\tilde{f_1} = \tilde{X} \delta = X\delta - \bar{1} \bar{1} X \frac{\delta}{n} = X\delta  - \bar{1}c = f_1 - c
$$
Para la constante $c=\bar{1}X\frac{\delta}{n}$.Se hace de forma análoga para $f_2$.
\end{observación}
Finalmente, notemos que el proceso de centrado por columnas reduce el rango a $k_1-1$, así que sólo se podrán estimar $k_1-1$ de los $k_1$ elementos de $\delta$ de forma única. Para solucionar este problema se retira una columna de $\tilde{X}$ y de $D_1$ y la correspondiente componente de $\delta$ se hace 0.

Gracias a este razonamiento, el modelo aditivo puede ser expresado como $Y = Z\beta + \epsilon$, 
donde $Z = (\bar{1},X,V)$ y $\beta = (\alpha,\delta,\gamma)^T$. De este modo, la penalización que añadimos al criterio de mínimos cuadrados es:
$$
\beta^T S_1 \beta = (\alpha,\delta^T,\gamma^T)
\begin{pmatrix}
0 & 0 & 0 \\ 0 & S_1 & 0 \\ 0 & 0 & 0
\end{pmatrix}
\begin{pmatrix} \alpha \\ \delta \\ \gamma \end{pmatrix} = 
\delta^T S_1 \delta
$$

**Ajuste del modelo aditivo por mínimos cuadrados penalizados**
Gracias a la expresión de la penalización que acabamos de obtener para el modelo aditivo $\ref{modelo aditivo}$ se tiene que la estimación de los coeficientes $\hat{\beta}$ del modelo se obtienen minimizando la función objetivo de mínimos cuadrados penalizados:
$$
||y- X\beta||^2 + \lambda_1 \beta^T S_1 \beta + \lambda_2 \beta^T S_2 \beta
$$
Donde $\lambda_1$ es el parámetro de suavizado que controla a $f_1$ y $\lambda_2$ el que controla a $f_2$. Entonces, los estimadores de $\beta$ son:
$$
\hat{\beta} = (X^TX + \lambda_1 S_1 \lambda_2 S_2)^{-1} X^TY
$$

## Modelos aditivos generalizados

Cómo ya hemos mencionado varias veces a lo largo del trabajo y como se indica en @Wood, en los MAG se quiere predecir algunas funcionasmonótonas que expresan la relación entre los predictores y el valor esperado de la variable de respuesta. Del mismo modo que para los MLG la variable de respuesta debe seguir una distribución de tipo exponencial, $\ref{Dist-exponencial}$.

Mientras que el modelo aditivo se estimaba mediante mínimos cuadrados penalizados, el MAG se estirmará utilizando máxima verosimilitud penalizada, aunque en la práctica se utiliza un algoritmo de iteración de mínimos cuadrados penalizados (PIRLS). Partimos dando la definión general del modelo como lo hace @Wood:

\begin{definición}[Estructura general del modelo aditivo generalizado]
Sean:
\begin{itemize}
  \item $Y$ un vector aleatorio n-dimensional, cuyas componentes siguen una distribución de tipo exponencial $y_i \backsim EF(\mu_i,\phi)  \hspace{0.2cm} \forall i=1,\dots,n$.
  \item $X$ una matriz de orden $n \times p$, $p \leq n$, de columnas $x_j \hspace{0.2cm} \forall j=1,\dots,p$ con constantes conocidas.
  \item $f_j: \mathbb{R} \rightarrow \mathbb{R} \hspace{0.2cm} \forall j = 1, \dots,p$ funciones desconocidas.
  \item $A$ la matriz asociada al modelo paramétrico de orden $n \times p$.
  \item $\gamma$ un vector de parámetros de dimensión n.
  \item $g: \mathbb{R} \rightarrow \mathbb{R}$ una función monótona y diferenciable.
\end{itemize}
Definimos el Modelo Aditivo Generalizado como:
\begin{equation}
g(\mu_i) = A_i \gamma + \sum_{j=1}^p f_j(x_{ij})
\label{MAG}
\end{equation}
\end{definición}

@Wood también propone una definición alternativa utilizando distintas formas de $L_{ij}$:
$$
g(\mu_i) = A_i \gamma + \sum_{j=1}^p L_{ij}f_j(x_{j})
$$
La más básica de ella es: $L_{ij}f_j(x_j) = f_j(x_{ij})$ que da la forma habitual del MAG, pero existen otras variaciones de ellas y cada una tiene una utilidad específica.

Luego, para cada $f_j$ se deben elegir una base de suavizado y una penalización, las cuales dan lugar a las matrices modelo $X^{[j]}$ y a la matriz de penalización $S^{[j]}$, de modo que $X_{ik}^{[j]} = b_{jk}(x_{ji})$ para $b_{jk}$ la k-ésima función básica para $f_j$.

### Ajuste del modelo

Seguiremos las directrices dadas en @Wood para dar la estimación de los parámetros $\beta$ cuando suponemos dado los parámetros de suavizado $\lambda$. Partimos notando que el vector de parámetros $\beta$ contiene a $\gamma$ y a 