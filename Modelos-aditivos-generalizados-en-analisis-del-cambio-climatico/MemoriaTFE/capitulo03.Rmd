---
author: "Francisco José Lozano Ruiz"
date: "27/10/2017"
documentclass: book
forprint: true  # true: imprime a dos caras, false: libro digital
fontsize: 12pt # 10pt,11pt
geometry: margin = 2.5cm 
bibliography: ["bib/library.bib", "bib/paquetes.bib"]
# metodobib -> true: natbib (descomentar: citation_package: natbib) 
#           -> false: pandoc (comentar: citation_package: natbib)
metodobib: true
#natbib: plainnat, abbrvnat, unsrtnat
biblio-style: "plainnat"
#Método 2 (pandoc): descomente una línea de las 2 siguientes en caso de usarlo
csl: methods-in-ecology-and-evolution.csl      # no numera mejor en las citas
#csl: acm-sig-proceedings-long-author-list.csl  # numera peor en las citas
link-citations: yes
output: 
  pdf_document:
    keep_tex: no
    number_sections: yes
    citation_package: natbib  # comentado usa: pandoc-citeproc
    #toc: yes
    fig_caption: yes
    template: latex/templateMemoriaTFE.tex
    includes:
      #before_body: portadas/latex_paginatitulo_modTFE.tex
      #in_header: latex/latex_preambulo.tex
      #after_body: latex/latex_antes_enddoc.tex
---



```{r include=FALSE}
knitr::opts_chunk$set(fig.path = 'figurasR/',
                      echo = FALSE, warning = FALSE, message = FALSE,
                      fig.pos="H",fig.align="center",out.width="95%",
                      cache=FALSE)

```


<!-- \setcounter{chapter}{2} -->
<!-- \setcounter{chapter}{2} escribir 2 para capítulo 3  -->
<!-- \pagenumbering{arabic} -->

\ifdefined\ifprincipal
\else
\setlength{\parindent}{1em}
\pagestyle{fancy}
\setcounter{tocdepth}{4}
\tableofcontents
<!-- \nocite{*} -->
\fi

\ifdefined\ifdoblecara
\fancyhead{}{}
\fancyhead[LE,RO]{\scriptsize\rightmark}
\fancyfoot[LO,RE]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[LE,RO]{\footnotesize\thepage}
\else
\fancyhead{}{}
\fancyhead[RO]{\scriptsize\rightmark}
\fancyfoot[LO]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[RO]{\footnotesize\thepage}
\fi
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\theoremstyle{remark}
\newtheorem{observación}{Observación}[section]
\theoremstyle{definition}
\newtheorem{definición}{Definición}[section]
\theoremstyle{definición}
\newtheorem{proposición}{Proposición}[section]
\theoremstyle{remark}
\newtheorem{ejemplo}{Ejemplo}[section]

# Modelos aditivos generalizados

## Introducción

Como bien podemos intuir por su nombre, los modelos aditivos generalizados no son más que la fusión entre los modelos lineales generalizados y los modelos aditivos, los cuales se introducen con una sección en este capítulo. Podemos ver estos dos tipos de modelos como extensiones del modelo lineal. Por un lado, como vimos en el capítulo anterior, el MLG hace uso de una función de enlace entre el predictor lineal y el valor esperado de la variable dependiente para poder expresar relaciones más complejas y relaja la hipótesis distribucional permitiendo que tal variable siga distribuciones de la familia exponencial. Por otro lado, los modelos aditivos, además de también relajar esta hipótesis de distribución, introducen las funciones de suavizado en el modelo, estas proporcionan más flexibilidad a la hora de relacionar las variables explicativas con la de respuesta.

Luego, como ya hemos mencionado, y como se plantea en @Hastie, el MAG reune estas dos propuestas de modo que generaliza el modelo aditivo de la misma forma que el MLG generalizaba el modelo lineal. Sin embargo, la flexibilidad que proporciona este modelo da lugar a dos nuevos problemas teóricos: cómo estimar las funciones de suavizado y cómo de "suaves" deben ser. 

En este capítulo nos adentramos en los modelos no paramétricos, es decir, en aquellos que en vez de expresar la relación del valor esperado de la variable de respuesta con las variables predictoras mediante un predictor lineal, lo hacen mediante funciones $f$, como se vió en $\ref{eq:modelo general}$, pero ahora sin hacer ninguna suposición sobre ella. Esto conllevará en muchas ocasiones un mejor ajuste del modelo y traerá a la mesa una nueva cuestión conocida como sobreajuste que, aunque ya aparecía para los modelos paramétricos, ahora jugará un papel fundamental a la hora de querer predecir datos fuera de los observados. Este concepto refleja el hecho de que el modelo ajusta tan bien los datos proporcionados para la estimación de sus parámetros que es incapaz de mostrar la verdadera relación entre las variables que se estudian y, por tanto, da lugar a predicciones de nuevos datos que no serán las idóneas.

Tal y como se hace en @Wood, comenzaremos viendo cómo construir los modelos aditivos generalizados, es decir, qué bases de funciones podemos elegir para obtener las funciones de suavizado y qué parámetro de suavizado se debe seleccionar o cómo se puede estimar. Luego se introduce el modelo aditivo, en el que se utilizarán los resultados vistos a lo largo del capítulo. Tras todo ello se propone la forma final del modelo aditivo generalizado.


\begin{definición}[Estructura básica del modelo aditivo generalizado]
$$
\mu = \begin{pmatrix} \mu_1 \\ \vdots \\ \mu_n \end{pmatrix} = 
\begin{pmatrix} E[Y_1] \\ \vdots \\ E[Y_n] \end{pmatrix} = E[Y]
$$
\begin{equation}
g(\mu_i) = A_i\theta + f_1(x_{1i} + f_2(x_{2i}) + f_3(x_{3i},x_{4i}) + \dots  \hspace{0.5cm}, 
\forall i =1,\dots,n
\label{eq:MAG}
\end{equation}
Donde:
\begin{itemize}
  \item $Y_i$ es la variable de respuesta y sigue una distribución de la familia exponencial de media $\mu_i$ y parámetro de escalado $\phi$. A partir de ahora esto lo denotaremos por: $Y_i \backsim EF(\mu_i,\phi)$.
  \item $A_i$ es la fila i-ésima de la matriz del modelo para aquellas componentes del modelo que son estrictamente paramétricas.
  \item $\theta$ es el correspondiente vector de parámetro, que antes denotábamos por $\beta$, para las variables predictoras mencionadas en el anterior punto.
  \item Las $f_i$ son las funciones de suavizado para las covariables $x_k$.
\end{itemize}
\end{definición}


## Suavizado univariante

Dicho esto, partiremos considerando modelos que, aunque no sean adecuados para un uso práctico general, nos permitirán estudiar el marco teórico de una forma más sencilla. Es decir, en esta sección consideraremos un modelo con una sola función de suavizado, $f$, y una sola covariable, $x$, de la forma: 
\begin{equation}
y_i = f(x_i) + \epsilon_i
\label{eq: basic MAG}
\end{equation}
Donde $y_i$ es la variable de respuesta y los $\epsilon_i$ son variables aleatorias independientes e identicamente distribuidas como $N(0,\sigma^2)$ que representan el error.


### Bases de funciones

Nos proponemos en esta sección obtener una estimación de la función de suavizado a partir de una base de un espacio de funciones, en el que también se encontrará $f$ (o una aproximación suya). Elegir una base equivale a tomar un conjunto de funciones $\{b_j(x)\}_{j=1}^k$ y, por tanto, podemos representar la función de suavizado como:
\begin{equation}
f(x) = \sum_{j=1}^k b_j(x)\beta_j
\label{eq:base funcion suavizado}
\end{equation}
Para ciertos parámetros $\beta_j$ a determinar.

**Base polinómica** \newline
Si consideramos la base $\mathcal{B}$ del espacio de polinomios de grado k, es decir, $\mathcal{B} = \{1, x_i, x_i^2, \dots, x_i^k \}$, la función de suavizado toma la forma: 
$$
f(x) = \beta_1 + \beta_2 x + \beta_3 x^2 + \dots + \beta_{k+1} x^k
$$
Y, por tanto, el modelo $\ref{eq: basic MAG}$ queda:
$$
y_i = \beta_1 + \beta_2 x_i + \beta_3 x_i^2 + \dots + \beta_{k+1} x_i^k + \epsilon_i
$$

\begin{observación}[Problema de la base polinómica]
Notemos que por el teorema de Taylor, la base polinomial nos será útil cuando nuestro interés sea el de estudiar las propiedades de la función de suavizado en el entorno de un punto concreto, pero nos encontramos con problemas cuando queremos hacerlo en todo el dominio de $f$.

El principal problema se debe a que la interpolación de los datos puede resultar en una función muy oscilante o que no ajuste bien la indormación, dependiendo del valor de k. Esto se puede solucionar de cierta manera con el siguiente tipo de base de funciones.
\end{observación}

**Base lineal por partes** \newline
Consideremos ahora una partición de nodos $\{x^*_j : j = 1,\dots,k \}$ del rango de la variable predictora $x$ tal que $x^*_j > x^*_{j+1}$ y la base de funciones $\mathcal{B} = \{b_j(x)\}_{j=1}^k$ donde:


$$
b_1(x) = \left\{
\begin{array}{ l }
\frac {x^*_2 - x} {x^*_2 - x^*_1} \hspace{0.5cm}, si \hspace{0.2cm} x < x^*_2  \\
0 \hspace{0.5cm} c.c.
\end{array}
\right. 
$$
$$
b_j(x) = \left\{
\begin{array}{ l }
\frac {x - x^*_{j-1}} {x^*_j - x^*_{j-1}} \hspace{0.5cm}, si \hspace{0.2cm} x^*_{j-1}<x < x^*_j  \\
\frac {x^*_{j+1} - x} {x^*_{j+1} - x^*_j} \hspace{0.5cm}, si \hspace{0.2cm} x^*_{j}<x < x^*_{j+1} \\
0 \hspace{0.5cm} c.c
\end{array}
\right.
$$

$$
b_k(x) = \left\{
\begin{array}{ l }
\frac {x - x^*_{k-1}} {x^*_k - x^*_{k-1}} \hspace{0.5cm}, si \hspace{0.2cm} x > x^*_{k-1}  \\
0 \hspace{0.5cm} c.c.
\end{array}
\right. 
$$
Es decir, la base de funciones $b_j(x)$ que son 0 en todo su dominio excepto entre los nodos a izquierda y derecha de $x_j^*$, donde crece y decrece de forma lineal hasta llegar a 1 en tal nodo. Este tipo de funciones se conocen como *tent functions*.

\begin{ejemplo}
Supongamos que el rango de $x$ va de 0 a 5 y consideremos 6 nodos: $\{0,1,2,3,4,5\}$, entonces podemos representar las funciones $b_0(x)$, $b_2(x)$ y $b_5(x)$ como:
```{r}
b0 <- function(x) {
  ifelse(x < 1, 1-x, 0)
}

b2 <- function(x) {
  ifelse(1< x & x < 2, x-1, ifelse(2<x & x<3,3-x,0))
}

b5 <- function(x){
  ifelse(x > 4, x-4, 0)
}

x <- seq(0, 5, length.out = 100)
y0 <- b0(x)
y2 <- b2(x)
y5 <- b5(x)

par(mfrow = c(1, 3))

plot(x, y0, type = "l", col = "blue", main = "Función básica b0(x)")
abline(v = 0, lty = 2, col = "green")
points(0, 0, col = "red", pch = 16)
text(0, 0, "x0", pos = 3)
plot(x, y2, type = "l", col = "blue", main = "Función básica b2(x)")
abline(v = 2, lty = 2, col = "green")
points(2, 0, col = "red", pch = 16)
text(2, 0, "x2", pos = 3)
plot(x, y5, type = "l", col = "blue", main = "Función básica b5(x)")
abline(v = 5, lty = 2, col = "green")
points(5, 0, col = "red", pch = 16)
text(5, 0, "x5", pos = 3)

```
\end{ejemplo}

De momento sólo planteamos estas formas de estimar las funciones de suavizado para tener una idea inicial y sencilla de cómo hacerlo pero más adelante dedicamos una sección a mejorar estas estimaciones mediante *splines*.

### Control del suavizado

Nos interesará ahora controlar el grado de suavizado del GAM. Para ello tendremos en cuenta que el modelo aproxime de forma correcta los datos a la vez que la curvatura se mantiene controlada. Consideramos un nuevo parámetro $\lambda$, denominado parámetro de suavizado, el cuál tiene como principal función el compensar entre la fidelidad a los datos del modelos y el grado de suavizado del mismo.

Notemos primero que podemos representar la penalización a la curvatura de $f$ como: 
$$
\int(f'')^2
$$
Y en el caso de utilizar la base de funciones lineales por partes se puede aproximar[^1] por:
$$
\sum_{j=2}^{k-1}(f(x_{j-1}^*)-2f(x_j^*)+f(x_{j+1}^*))^2
$$

Es fácil observar que cuando $f$ es una línea recta la penalización es 0 y cuando presenta muchas fluctuaciones en su curvatura este término es mayor.

[^1]: Se supone que se los nodos están espaciados de manera uniforme, pues en el caso de no que no lo estuvieran habría que añadir pesos a la suma.

Luego, en vez de ajustar el modelo por mínimos cuadrados, ahora se hará añadiendo la anterior penalización, es decir, minimizando: 
$$
||y - X\beta||^2 + \lambda \sum_{j=2}^{k-1}(f(x_{j-1}^*)-2f(x_j^*)+f(x_{j+1}^*))^2
$$

\begin{observación}
Mientras mayor sea $\lambda$ más importancia le estaremos dando a que la función $f$ sea suave y menos a que a aproxime bien los datos. De hecho, cuando $\lambda \rightarrow \infty $ la función de suavizado $f$ que minimiza la anterior expresión será una línea recta y cuando $\lambda=0$ resultará en una estimación no penalizada. 
\end{observación}

### Elección del parámetro de suavizado

Cómo hemos visto en la observación anterior: si el parámetro de suavizado es muy grande, el modelo será demasiado simple como para ajustarse bien a los datos y si es muy pequeño, la función de suavizado tendrá una curvatura muy alta. En cualquiera de los casos se tendrá que la estimación de $f$ no se parecerá a la función real que ajusta los datos. Por ello, debemos dar un criterio para la elección de $\lambda$.

Un primer criterio planteado en @Wood es el de elegir $\lambda$ de forma que minimice la siguiente expresión para $x_1,\dots,x_n$ unas observaciones dadas.
$$
M = \frac{1}{n} \sum_{i=1}^n(\hat{f_i}-f_i)^2
$$
Donde $\hat{f_i} = \hat{f}(x_i)$ es la evaluación de los puntos dados en la estimación de la función y $f_i = f(x_i)$ son sus evaluaciones en la función real.

Sin embargo, como la función $f$ es desconocida, no es posible utilizar este criterio directamente. Daremos entonces una primera versión **método de validación cruzada**.

\begin{definición}[Validación cruzada ordinaria]
Sea $\hat{f_i}^{[-i]}$ la estimación de la función de suavizado que ajustada por todos los datos $\{(x_j,y_j)\}_{j=1}^n$ menos el i-ésimo, se define la validación cruzada ordinaria como:
\begin{equation}
\nu_0 = \frac{1}{n} \sum_{i=1}^n(\hat{f_i}^{[-i]}-y_i)^2
\label{cross-val-ord}
\end{equation}

\end{definición}

Se puede entender como que se ajusta el modelo sin utilizar la observación $(x_i,y_i)$, se predice la variable de respuesta con este modelo en el punto $x_i$ y luego se calcula la diferencia al cuadrado entre la estimación y el valor observado $\forall i=1,\dots ,n$.

\begin{observación}
Podemos ver que tomar $\lambda$ de modo que minimice $\nu_0$ es una buena manera de abordar que minimice M. Para ello veamos que $E[\nu_0] \approx E[M] + \sigma^2$. Sustituyendo en $\ref{cross-val-ord}$ que $y_i = f_i + \epsilon_i$ nos queda que:
$$
\nu_0 = \frac{1}{n} \sum_{i=1}^n(\hat{f_i}^{[-i]}-f_i + \epsilon_i)^2 = 
        \frac{1}{n} \sum_{i=1}^n[(\hat{f_i}^{[-i]}-f_i)2 - 2(\hat{f_i}^{[-i]}-f_i)\epsilon_i + \epsilon_i^2]
$$
Entonces, tomando valor esperado y teniendo en cuenta que $E[\epsilon_i]=0$ y que $\epsilon_i$ y $f_i$ son independientes:
$$
E[\nu_0] = \frac{1}{n} E[\sum_{i=1}^n(\hat{f_i}^{[-i]}-f_i)^2] + \sigma^2 = E[M] + \sigma^2
$$
Por lo tanto, cuando $n \rightarrow \infty$ se tienen las igualdades $E[\nu_0] = E[M] + \sigma^2$ y $\hat{f}^{[-i]} = \hat{f}$.
\end{observación}

Si los modelos sólo fueran juzgados por su capacidad de ajustar los datos que les aportamos, entonces siempre se elegirían los modelos más complejos, pero el elegir el modelo que maximice la capacidad de predecir nuevos datos no tiene este problema.

Sin embargo, como se indica en @Wood, p.171, este método es costoso computacionalmente, ya que se deben realizar n ajustes de los datos, por ello se propone un nuevo método el cuál hace uso de la matriz de influencia $A$.

\begin{definición}[Validación cruzada generalizada]
Dadas unas observaciones $\{(x_i,y_i)\}_{i=1}^n$ se elige $\lambda$ tal que minimice:
$$
\nu_g = n \frac{\sum_{i=1}^n(y_i-\hat{f_i})^2}{(n-tr(A))^2}
$$
\end{definición}

## Modelos aditivos

